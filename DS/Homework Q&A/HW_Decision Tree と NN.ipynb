{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f08b890e-abbb-4d85-a25e-868148a9953c",
   "metadata": {},
   "source": [
    "1. Decision tree"
   ]
  },
  {
   "cell_type": "raw",
   "id": "22d9b6c7-ad10-4db4-bfdb-b7c62a72cddb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "722d0ca5-7a6e-48c9-824b-94da39f4f181",
   "metadata": {},
   "source": [
    "注释以下每行代码的意思。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b33feb4-b59f-4c0e-a3a5-2848e5d71dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valueOfMaxProb(vals):\n",
    "    tarray=np.unique(vals, return_counts=True) #calculate the unique value in vals and also return # of time each appears.\n",
    "    imax=np.argmax(tarray[1]) #choose the index of the max # of the time of appearance of unique vals.\n",
    "    return tarray[0][imax], tarray[1][imax]/vals.shape[0] #return the respective value, the ratio to the total # of samples.\n",
    "\n",
    "def value_count(vals):\n",
    "    return np.unique(vals, return_counts=True) #return the unique value in vals and also return # of time each appears.\n",
    "\n",
    "def entropy(cls): ## after a series testing, this is by far the fastest\n",
    "    tarray=np.unique(cls, return_counts=True) # calculate the unique value in cls and also return # of time each appears.\n",
    "    ent=0.0 #initialize with 0.0\n",
    "    for count in tarray[1]: # for loop for the number of appearance for each unique value.\n",
    "        ratio=count/cls.shape[0]  # number of appearance over number of samples. (ratio)\n",
    "        ent+=ratio*math.log(ratio,2) # add to the partial entropy to the total.\n",
    "    return -ent # return the -entropy\n",
    "\n",
    "def gini(cls): ## after a series testing, this is by far the fastest\n",
    "    tarray=np.unique(cls, return_counts=True) # calculate the unique value in cls and also return # of time each appears.\n",
    "    size=cls.shape[0] # number of samples\n",
    "    score=0.0 # initialize the lost score\n",
    "    for count in tarray[1]: # for loop w.r.t. number of appearance for each unique values.\n",
    "        p=count/size # calc the prior probability \n",
    "        score+=p*p # add it to the total loss\n",
    "    return 1.0-score #return gini score.\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, toPrint=False, measurement=entropy, LeafNodeContent=value_count):\n",
    "        self.theTree=dict()  #initialize the tree\n",
    "        self.toPrint=toPrint  #print info or not\n",
    "        self.LeafNodeContent=LeafNodeContent  # The info in the Leaf Node\n",
    "        self.measurement=measurement # measure to evaluate the goodness of split.\n",
    "                        \n",
    "    def trySplit(self, df,attrib, iscontinuous=False):\n",
    "        ntotal=df.shape[0] # number of data points.\n",
    "        if iscontinuous:  # for continuous feature\n",
    "            b_score, b_value=999, 0 # initialize the entropy and threshold\n",
    "            for value in set(df[attrib]): # for loop for each unique value of this feature\n",
    "                df[self.workingVector]=df[attrib]<=value  #transform continous value to binary bool value\n",
    "                score=0.0 #initialize the entropy\n",
    "                for v, nodei in df.groupby(self.workingVector): # for each value of the feature and its respective row of df\n",
    "                    score+=float(nodei.shape[0]/ntotal)*self.measurement(nodei[self.outcomeVar]) # add the entropy (ratio*partial_entropy) to total\n",
    "                if score<b_score: # choose the best splitting node\n",
    "                    b_score, b_value= score, value\n",
    "            return b_score, b_value\n",
    "        else: #for categorical data\n",
    "            score=0.0 #same as above for the else case\n",
    "            for v, nodei in df.groupby(attrib):\n",
    "                score+=float(nodei.shape[0]/ntotal)*self.measurement(nodei[self.outcomeVar])\n",
    "            return score, None\n",
    "            \n",
    "    def getMostScoreAttrib(self,df):   ## the score include InformationGain and OverallGiniScore\n",
    "        optScore, optValue, optVar =999, 0, ''  #initialize the entropy, threshold, feature\n",
    "        for v in df.columns: # for each column\n",
    "            if v!=self.outcomeVar and v!=self.workingVector: ## outcomeVar should be avoided, so does the temperary variable\n",
    "                score, svalue =self.trySplit(df,v, v in self.continuousVars) # get the score for this split\n",
    "                if score<optScore: #if the entropy is less than the ealier one\n",
    "                    optScore, optValue, optVar=score, svalue, v # get the best one\n",
    "        return optVar, optValue    ## the argmin is what we need\n",
    "\n",
    "    def splitNodes(self, df, treeNode):\n",
    "        attrib, svalue=self.getMostScoreAttrib(df)  # get the most suitable feature for split\n",
    "        tattrib=(attrib, svalue)  # feature and the threshold\n",
    "        treeNode[tattrib]={} # for this feature, add a new dictionary\n",
    "        \n",
    "        if self.toPrint: print(\"\\n>>>>> split by \",tattrib, '\\n', df) #to print info\n",
    "\n",
    "        if svalue==None: # for categorical feature\n",
    "            broupbyExpre=df[attrib] # get this feature column\n",
    "        else:\n",
    "            broupbyExpre=df[attrib]<svalue # for continuous feature \n",
    "\n",
    "        for v, subdf in df.groupby(broupbyExpre): # for each unique value and the respective group\n",
    "            if toPrint: print(\"\\n>>>>> split by \",tattrib, v, '\\n', subdf) # print info\n",
    "            theScore=self.measurement(subdf[self.outcomeVar])  # get the sub-entropy \n",
    "            if theScore!=0 and subdf.shape[1]>3:  # rule out the case for (target already same) and (the remaining feature # only 1)\n",
    "                if self.toPrint:  #print info\n",
    "                    print(tattrib, '=',v, theScore,'. Calling splitNodes() recursively')\n",
    "                    print(subdf)\n",
    "                treeNode[tattrib][v]={} # give a new dic to this value of this feature\n",
    "                self.splitNodes(subdf.drop(attrib, axis=1), treeNode[tattrib][v]) \n",
    "                # iteration: from the remaining feature, choose the best one to split from the given node.\n",
    "            else: # come to leaf nodes\n",
    "                if self.toPrint: print(tattrib,'=', v, 'leaf node', [(v.shape[0],k) for k,v in subdf.groupby(self.outcomeVar)])                 \n",
    "                treeNode[tattrib][v]=self.LeafNodeContent(subdf[self.outcomeVar]) # give the value_count to this feature node's key \n",
    "        del df  # don't forget to remove the dataset\n",
    "\n",
    "    def fit(self, X_train, y_train, continuousVars=[]):\n",
    "        self.outcomeVar=y_train.name  # store the target col's name\n",
    "        self.continuousVars=continuousVars # store continuous features \n",
    "\n",
    "        self.workingVector='_';  ## assume the working vector has name '_'\n",
    "        while self.workingVector in X_train.columns: self.workingVector+='_'  ## adding more '_' if needed\n",
    "\n",
    "        self.labels=np.unique(y_train)  ## store the total categories of target labels\n",
    "        XY_train=X_train.merge(y_train, left_index=True, right_index=True) #merge data and label together\n",
    "        XY_train[self.workingVector]=False  ## prefill the working vector, from now on no new vector \n",
    "        self.theTree=dict() # initialize the tree\n",
    "        self.splitNodes(XY_train, self.theTree)  # call the split function to train the tree.\n",
    "        \n",
    "    def totalValueCount(self,di):\n",
    "        outs=pd.Series()  # initialize an empty Series\n",
    "        for key in di: # for loop of keys\n",
    "            vals =di[key] # get the values of the key\n",
    "            if type(vals)==dict: # if the value is also a dic\n",
    "                vals=self.totalValueCount(vals) # recall this function to calc the new dic\n",
    "            ss=pd.Series(vals[1], index=vals[0]).fillna(0) # use the target_count as the value, with index the target, also fill NaN with 0 \n",
    "            outs=outs.add(ss,fill_value=0) # 如果索引存在于两个 Series 中，则相加。\n",
    "        return list(outs.index), outs.values # return the index and values\n",
    "        \n",
    "    def estimate(self, dic, x_test):\n",
    "        if len(dic)==0: return # if dic is empty, then return empty\n",
    "        node=next(iter(dic)) # iteratively get the top keys. \n",
    "        field=node[0] # （特征和阈值的元组） we get the feature \n",
    "        if field not in x_test: return  # 如果测试样本中没有当前特征，直接返回 None\n",
    "        if node[1]==None:   # if it's categorical         \n",
    "            match=dic[node].get(x_test[field]) # if they are equal, return the equal part('s value).\n",
    "        else: # if it's continuous\n",
    "            match=dic[node].get(x_test[field]<node[1])  # firstly transform to binary boolean value.\n",
    "        if match is None: # if didn't find a match \n",
    "            return self.totalValueCount(dic[node]) # return the total value count for each value under this node.\n",
    "            \n",
    "        if type(match) is dict: #if this match is still a dict\n",
    "            if self.toPrint: print(match)  # match is a shorted dictionary so it can be treated just like the dic\n",
    "            return self.estimate(match, x_test)  # recall this function to furthur match\n",
    "        else:\n",
    "            if self.toPrint: print(match, 'is a leaf of type ', type(match)) \n",
    "            return match # or else return this non-dictionary match\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        predicted=[] # init an empty list to store pred\n",
    "        for i in range(X_test.shape[0]): # for each data point in test set\n",
    "            pred=self.estimate(self.theTree, X_test.iloc[i,:]) # try to find matches between the tree and the test point.\n",
    "            if pred==None: # if didn't find match\n",
    "                predicted.append(None) # just say none\n",
    "            else:\n",
    "                imax=np.argmax(pred[1]) # choose the match with the highest frequency, get the index\n",
    "                predicted.append(pred[0][imax]) # get the target from the index\n",
    "        return np.array(predicted)\n",
    "\n",
    "    def predict_proba(self, X_test):\n",
    "        predicted=pd.DataFrame([], columns=self.labels) # initialize an empty DF\n",
    "        for i in range(X_test.shape[0]): # for each test data point\n",
    "            pred=self.estimate(self.theTree, X_test.iloc[i,:]) # find match for this point\n",
    "            if pred==None: # if didn't find one\n",
    "                print(i, end=' ')\n",
    "                row=pd.Series([0]*len(self.labels), index=self.labels)  # let this row empty\n",
    "            else: # if find a match\n",
    "                row=pd.Series(pred[1]/pred[1].sum(), index=pred[0])  # get the probability for each label belonging\n",
    "            predicted=predicted.append(row, ignore_index=True) \n",
    "        return np.array(predicted.fillna(0))  ## fill the missing with 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22057a0c-4932-417e-9746-cbafe7056a7c",
   "metadata": {},
   "source": [
    "2. Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8390ca1-3917-442f-80ad-856753a1480c",
   "metadata": {},
   "source": [
    "在https://archive.ics.uci.edu/ml/index.php 选取一个你自己感兴趣的回归问题的数据集，在测试集（test set）上比较用pytorch训练的回归模型和线性回归这两种方法训练出来的模型的MPSE（mean prediction squared error）:\n",
    "$$MPSE=\\frac{1}{m}\\sum_{i=1}^{m}(\\hat{Y}_i-Y_i)^2,$$\n",
    "其中$\\hat{Y}_i$为测试集上的预测值，$Y_i$为测试集上的响应变量的值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d360ab-0992-4d22-9e1b-771afd2b60f5",
   "metadata": {},
   "source": [
    "I chose this dataset: https://archive.ics.uci.edu/dataset/206/relative+location+of+ct+slices+on+axial+axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c0f2d81-a65c-4c30-b5c8-e5fe577948b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patientId</th>\n",
       "      <th>value0</th>\n",
       "      <th>value1</th>\n",
       "      <th>value2</th>\n",
       "      <th>value3</th>\n",
       "      <th>value4</th>\n",
       "      <th>value5</th>\n",
       "      <th>value6</th>\n",
       "      <th>value7</th>\n",
       "      <th>value8</th>\n",
       "      <th>...</th>\n",
       "      <th>value375</th>\n",
       "      <th>value376</th>\n",
       "      <th>value377</th>\n",
       "      <th>value378</th>\n",
       "      <th>value379</th>\n",
       "      <th>value380</th>\n",
       "      <th>value381</th>\n",
       "      <th>value382</th>\n",
       "      <th>value383</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>-0.25000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.980381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>21.803851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>-0.25000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.977008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>21.745726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>-0.25000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.977008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>21.687600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>-0.25000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.977008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>21.629474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>-0.25000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.976833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>21.571348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53495</th>\n",
       "      <td>96</td>\n",
       "      <td>0.591906</td>\n",
       "      <td>0.357764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.552321</td>\n",
       "      <td>0.795304</td>\n",
       "      <td>0.946697</td>\n",
       "      <td>0.952227</td>\n",
       "      <td>0.84395</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29.290398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53496</th>\n",
       "      <td>96</td>\n",
       "      <td>0.612313</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.864160</td>\n",
       "      <td>0.820531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.938813</td>\n",
       "      <td>0.94374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>27.945721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53497</th>\n",
       "      <td>96</td>\n",
       "      <td>0.612313</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.864160</td>\n",
       "      <td>0.820531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.938813</td>\n",
       "      <td>0.94374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>27.945721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53498</th>\n",
       "      <td>96</td>\n",
       "      <td>0.634921</td>\n",
       "      <td>0.904555</td>\n",
       "      <td>0.956087</td>\n",
       "      <td>0.980208</td>\n",
       "      <td>0.157664</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>-0.25000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.994967</td>\n",
       "      <td>0.806688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>14.582997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53499</th>\n",
       "      <td>96</td>\n",
       "      <td>0.654321</td>\n",
       "      <td>0.891021</td>\n",
       "      <td>0.882244</td>\n",
       "      <td>0.979282</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>-0.25000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.994671</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>14.498955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53500 rows × 386 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       patientId    value0    value1    value2    value3    value4    value5  \\\n",
       "0              0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1              0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2              0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3              0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4              0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "53495         96  0.591906  0.357764  0.000000  0.000000  0.552321  0.795304   \n",
       "53496         96  0.612313  0.000000  0.000000  0.000000  0.864160  0.820531   \n",
       "53497         96  0.612313  0.000000  0.000000  0.000000  0.864160  0.820531   \n",
       "53498         96  0.634921  0.904555  0.956087  0.980208  0.157664  0.000000   \n",
       "53499         96  0.654321  0.891021  0.882244  0.979282  0.000000  0.000000   \n",
       "\n",
       "         value6    value7   value8  ...  value375  value376  value377  \\\n",
       "0     -0.250000 -0.250000 -0.25000  ...     -0.25  0.980381       0.0   \n",
       "1     -0.250000 -0.250000 -0.25000  ...     -0.25  0.977008       0.0   \n",
       "2     -0.250000 -0.250000 -0.25000  ...     -0.25  0.977008       0.0   \n",
       "3     -0.250000 -0.250000 -0.25000  ...     -0.25  0.977008       0.0   \n",
       "4     -0.250000 -0.250000 -0.25000  ...     -0.25  0.976833       0.0   \n",
       "...         ...       ...      ...  ...       ...       ...       ...   \n",
       "53495  0.946697  0.952227  0.84395  ...      0.00  0.000000       0.0   \n",
       "53496  0.000000  0.938813  0.94374  ...      0.00  0.000000       0.0   \n",
       "53497  0.000000  0.938813  0.94374  ...      0.00  0.000000       0.0   \n",
       "53498 -0.250000 -0.250000 -0.25000  ...     -0.25  0.000000       0.0   \n",
       "53499 -0.250000 -0.250000 -0.25000  ...     -0.25  0.000000       0.0   \n",
       "\n",
       "       value378  value379  value380  value381  value382  value383  reference  \n",
       "0      0.000000  0.000000       0.0       0.0     -0.25     -0.25  21.803851  \n",
       "1      0.000000  0.000000       0.0       0.0     -0.25     -0.25  21.745726  \n",
       "2      0.000000  0.000000       0.0       0.0     -0.25     -0.25  21.687600  \n",
       "3      0.000000  0.000000       0.0       0.0     -0.25     -0.25  21.629474  \n",
       "4      0.000000  0.000000       0.0       0.0     -0.25     -0.25  21.571348  \n",
       "...         ...       ...       ...       ...       ...       ...        ...  \n",
       "53495  0.000000  0.000000       0.0       0.0      0.00      0.00  29.290398  \n",
       "53496  0.000000  0.000000       0.0       0.0      0.00      0.00  27.945721  \n",
       "53497  0.000000  0.000000       0.0       0.0      0.00      0.00  27.945721  \n",
       "53498  0.994967  0.806688       0.0       0.0     -0.25     -0.25  14.582997  \n",
       "53499  0.994671  0.000000       0.0       0.0     -0.25     -0.25  14.498955  \n",
       "\n",
       "[53500 rows x 386 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('/Users/dongwenou/Downloads/Intro to DS/slice_localization_data.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "807ea030-56f4-4031-abcd-0884dab5dbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# Features (X) and target variable (y)\n",
    "X = data.drop(columns=['reference', 'patientId'])  # Exclude target and patient ID\n",
    "y = data['reference']  # The target variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17335abe-8f1a-4cde-87c3-1e5b017f1e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=369)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert target to numpy array\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9d288f2-8130-4193-b62e-36cd9ca9a2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression MPSE: 68.0326\n"
     ]
    }
   ],
   "source": [
    "# Train Linear Regression model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "# Calculate MPSE\n",
    "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
    "print(f\"Linear Regression MPSE: {mse_lr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9678085c-bba2-471f-85c3-d0d92f616d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "40125\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train))\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d54fea-d7fd-4fa2-b140-2f7905bc1d29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2093b389-26a6-46fa-93a6-1e165c20ae05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6953ef8b-ed8b-4efd-859e-01afc9c4d180",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 2010.9114\n",
      "Epoch [2/100], Loss: 210.7620\n",
      "Epoch [3/100], Loss: 80.2516\n",
      "Epoch [4/100], Loss: 58.4433\n",
      "Epoch [5/100], Loss: 46.5438\n",
      "Epoch [6/100], Loss: 38.8346\n",
      "Epoch [7/100], Loss: 33.1491\n",
      "Epoch [8/100], Loss: 28.7339\n",
      "Epoch [9/100], Loss: 25.6306\n",
      "Epoch [10/100], Loss: 23.0647\n",
      "Epoch [11/100], Loss: 21.2434\n",
      "Epoch [12/100], Loss: 19.5533\n",
      "Epoch [13/100], Loss: 18.3677\n",
      "Epoch [14/100], Loss: 17.1875\n",
      "Epoch [15/100], Loss: 16.1351\n",
      "Epoch [16/100], Loss: 15.2285\n",
      "Epoch [17/100], Loss: 14.3782\n",
      "Epoch [18/100], Loss: 13.2729\n",
      "Epoch [19/100], Loss: 12.0439\n",
      "Epoch [20/100], Loss: 11.2485\n",
      "Epoch [21/100], Loss: 9.9805\n",
      "Epoch [22/100], Loss: 9.0367\n",
      "Epoch [23/100], Loss: 8.1473\n",
      "Epoch [24/100], Loss: 7.3828\n",
      "Epoch [25/100], Loss: 6.6815\n",
      "Epoch [26/100], Loss: 6.0694\n",
      "Epoch [27/100], Loss: 5.5393\n",
      "Epoch [28/100], Loss: 5.1026\n",
      "Epoch [29/100], Loss: 4.6931\n",
      "Epoch [30/100], Loss: 4.3528\n",
      "Epoch [31/100], Loss: 4.0591\n",
      "Epoch [32/100], Loss: 3.8188\n",
      "Epoch [33/100], Loss: 3.5706\n",
      "Epoch [34/100], Loss: 3.3714\n",
      "Epoch [35/100], Loss: 3.1518\n",
      "Epoch [36/100], Loss: 2.9742\n",
      "Epoch [37/100], Loss: 2.8236\n",
      "Epoch [38/100], Loss: 2.6817\n",
      "Epoch [39/100], Loss: 2.5397\n",
      "Epoch [40/100], Loss: 2.4202\n",
      "Epoch [41/100], Loss: 2.3103\n",
      "Epoch [42/100], Loss: 2.2207\n",
      "Epoch [43/100], Loss: 2.1365\n",
      "Epoch [44/100], Loss: 2.0445\n",
      "Epoch [45/100], Loss: 1.9450\n",
      "Epoch [46/100], Loss: 1.8848\n",
      "Epoch [47/100], Loss: 1.8044\n",
      "Epoch [48/100], Loss: 1.7449\n",
      "Epoch [49/100], Loss: 1.6661\n",
      "Epoch [50/100], Loss: 1.5977\n",
      "Epoch [51/100], Loss: 1.5436\n",
      "Epoch [52/100], Loss: 1.4968\n",
      "Epoch [53/100], Loss: 1.4235\n",
      "Epoch [54/100], Loss: 1.3726\n",
      "Epoch [55/100], Loss: 1.3323\n",
      "Epoch [56/100], Loss: 1.2943\n",
      "Epoch [57/100], Loss: 1.3139\n",
      "Epoch [58/100], Loss: 1.2112\n",
      "Epoch [59/100], Loss: 1.1840\n",
      "Epoch [60/100], Loss: 1.1376\n",
      "Epoch [61/100], Loss: 1.0950\n",
      "Epoch [62/100], Loss: 1.0836\n",
      "Epoch [63/100], Loss: 1.0552\n",
      "Epoch [64/100], Loss: 1.0307\n",
      "Epoch [65/100], Loss: 0.9864\n",
      "Epoch [66/100], Loss: 0.9520\n",
      "Epoch [67/100], Loss: 0.9252\n",
      "Epoch [68/100], Loss: 0.8995\n",
      "Epoch [69/100], Loss: 0.8902\n",
      "Epoch [70/100], Loss: 0.8515\n",
      "Epoch [71/100], Loss: 0.8350\n",
      "Epoch [72/100], Loss: 0.8090\n",
      "Epoch [73/100], Loss: 0.7927\n",
      "Epoch [74/100], Loss: 0.7792\n",
      "Epoch [75/100], Loss: 0.7633\n",
      "Epoch [76/100], Loss: 0.7484\n",
      "Epoch [77/100], Loss: 0.7168\n",
      "Epoch [78/100], Loss: 0.7099\n",
      "Epoch [79/100], Loss: 0.6883\n",
      "Epoch [80/100], Loss: 0.6990\n",
      "Epoch [81/100], Loss: 0.6650\n",
      "Epoch [82/100], Loss: 0.6500\n",
      "Epoch [83/100], Loss: 0.6390\n",
      "Epoch [84/100], Loss: 0.6218\n",
      "Epoch [85/100], Loss: 0.6222\n",
      "Epoch [86/100], Loss: 0.5986\n",
      "Epoch [87/100], Loss: 0.5869\n",
      "Epoch [88/100], Loss: 0.5689\n",
      "Epoch [89/100], Loss: 0.5625\n",
      "Epoch [90/100], Loss: 0.5537\n",
      "Epoch [91/100], Loss: 0.5487\n",
      "Epoch [92/100], Loss: 0.5313\n",
      "Epoch [93/100], Loss: 0.5189\n",
      "Epoch [94/100], Loss: 0.5397\n",
      "Epoch [95/100], Loss: 0.5040\n",
      "Epoch [96/100], Loss: 0.4951\n",
      "Epoch [97/100], Loss: 0.4814\n",
      "Epoch [98/100], Loss: 0.4759\n",
      "Epoch [99/100], Loss: 0.4710\n",
      "Epoch [100/100], Loss: 0.4592\n",
      "Test MSE Loss: 0.9268\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# 1. 数据准备：转换为 DataLoader\n",
    "batch_size = 1024  # 批次大小\n",
    "\n",
    "# 确保数据为 NumPy 数组\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train).reshape(-1, 1)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test).reshape(-1, 1)\n",
    "\n",
    "# 将数据转换为 TensorDataset\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n",
    "                              torch.tensor(y_train, dtype=torch.float32))\n",
    "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                             torch.tensor(y_test, dtype=torch.float32))\n",
    "\n",
    "# DataLoader 分批加载数据\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 2. 定义神经网络模型\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)  # 输入层 -> 隐藏层1\n",
    "        self.relu = nn.ReLU()                 # ReLU 激活函数\n",
    "        self.fc2 = nn.Linear(128, 64)         # 隐藏层1 -> 隐藏层2\n",
    "        self.fc3 = nn.Linear(64, 1)           # 隐藏层2 -> 输出层\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# 3. 初始化模型、损失函数和优化器\n",
    "input_dim = X_train.shape[1]  # 输入特征数\n",
    "model = NeuralNetwork(input_dim)\n",
    "\n",
    "criterion = nn.MSELoss()          # 使用均方误差损失函数 (回归问题)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam 优化器\n",
    "\n",
    "# 4. 训练模型\n",
    "num_epochs = 100  # 训练轮次\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # 设置模型为训练模式\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:  # 分批加载数据\n",
    "        # 前向传播\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # 每个 epoch 输出损失\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# 5. 评估模型\n",
    "model.eval()  # 设置模型为评估模式\n",
    "test_loss = 0.0\n",
    "\n",
    "with torch.no_grad():  # 在评估阶段不计算梯度\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "print(f\"Test MSE Loss: {test_loss/len(test_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2ee61b8-591b-436f-b5db-5530ba403dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Comparison:\n",
      "Linear Regression MPSE: 68.0326\n",
      "Deep Learning MPSE: 0.9268\n"
     ]
    }
   ],
   "source": [
    "print(\"Model Comparison:\")\n",
    "print(f\"Linear Regression MPSE: {mse_lr:.4f}\")\n",
    "print(f\"Deep Learning MPSE: {test_loss/len(test_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e903daab-509d-4401-846f-c9b29e42effd",
   "metadata": {},
   "source": [
    "    We can find that it's a great improvement from Linear Regression to Neural Network!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
